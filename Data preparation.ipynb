{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db2bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/10 09:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 09:41:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/10 09:41:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+---------+----------------+-------+\n",
      "|magnitude|  depth|latitude|longitude|       date_time|tsunami|\n",
      "+---------+-------+--------+---------+----------------+-------+\n",
      "|      7.0|   14.0| -9.7963|  159.596|22-11-2022 02:03|      1|\n",
      "|      6.9|   25.0| -4.9559|  100.738|18-11-2022 13:37|      0|\n",
      "|      7.0|  579.0|-20.0508| -178.346|12-11-2022 07:09|      1|\n",
      "|      7.3|   37.0|-19.2918| -172.129|11-11-2022 10:48|      1|\n",
      "|      6.6|624.464|-25.5948|  178.278|09-11-2022 10:14|      1|\n",
      "|      7.0|  660.0|-26.0442|  178.381|09-11-2022 09:51|      1|\n",
      "|      6.8|630.379|-25.9678|  178.363|09-11-2022 09:38|      1|\n",
      "|      6.7|   20.0|  7.6712| -82.3396|20-10-2022 11:57|      1|\n",
      "|      6.8|   20.0|   18.33| -102.913|22-09-2022 06:16|      1|\n",
      "|      7.6| 26.943| 18.3667| -103.252|19-09-2022 18:05|      1|\n",
      "|      6.9|   10.0| 23.1444|  121.307|18-09-2022 06:44|      1|\n",
      "|      6.5|   10.0|  23.029|  121.348|17-09-2022 13:41|      1|\n",
      "|      7.0|  137.0|-21.2077|  170.239|14-09-2022 11:04|      1|\n",
      "|      7.6|  116.0| -6.2237|  146.471|10-09-2022 23:47|      1|\n",
      "|      6.6|   12.0| 29.7263|  102.279|05-09-2022 04:52|      0|\n",
      "|      6.6|   30.0|-32.6922| -178.959|14-08-2022 13:44|      1|\n",
      "|      7.0| 33.729| 17.5978|  120.809|27-07-2022 00:43|      1|\n",
      "|      6.5| 622.73| -9.0618| -71.1647|08-06-2022 00:55|      0|\n",
      "|      7.2|  236.0|-14.8628| -70.3081|26-05-2022 12:02|      1|\n",
      "|      6.9|   10.0|-54.1325|  159.027|19-05-2022 10:13|      1|\n",
      "+---------+-------+--------+---------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()\n",
    "\n",
    "file_path = 'Dataset/earthquake_data.csv'\n",
    "earthquake_data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Selecting the desired columns\n",
    "selected_columns = ['magnitude', 'depth', 'latitude', 'longitude','date_time','tsunami']\n",
    "selected_data = earthquake_data.select(selected_columns)\n",
    "\n",
    "# Display the top rows of the selected data\n",
    "selected_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49729b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------------------+-------+--------------+--------+---------+\n",
      "|       date_time|magnitude|Magnitude Category|  depth|Depth Category|latitude|longitude|\n",
      "+----------------+---------+------------------+-------+--------------+--------+---------+\n",
      "|22-11-2022 02:03|      7.0|             Major|   14.0|       Shallow| -9.7963|  159.596|\n",
      "|18-11-2022 13:37|      6.9|            Strong|   25.0|       Shallow| -4.9559|  100.738|\n",
      "|12-11-2022 07:09|      7.0|             Major|  579.0|          Deep|-20.0508| -178.346|\n",
      "|11-11-2022 10:48|      7.3|             Major|   37.0|       Shallow|-19.2918| -172.129|\n",
      "|09-11-2022 10:14|      6.6|            Strong|624.464|          Deep|-25.5948|  178.278|\n",
      "|09-11-2022 09:51|      7.0|             Major|  660.0|          Deep|-26.0442|  178.381|\n",
      "|09-11-2022 09:38|      6.8|            Strong|630.379|          Deep|-25.9678|  178.363|\n",
      "|20-10-2022 11:57|      6.7|            Strong|   20.0|       Shallow|  7.6712| -82.3396|\n",
      "|22-09-2022 06:16|      6.8|            Strong|   20.0|       Shallow|   18.33| -102.913|\n",
      "|19-09-2022 18:05|      7.6|             Major| 26.943|       Shallow| 18.3667| -103.252|\n",
      "|18-09-2022 06:44|      6.9|            Strong|   10.0|       Shallow| 23.1444|  121.307|\n",
      "|17-09-2022 13:41|      6.5|            Strong|   10.0|       Shallow|  23.029|  121.348|\n",
      "|14-09-2022 11:04|      7.0|             Major|  137.0|  Intermediate|-21.2077|  170.239|\n",
      "|10-09-2022 23:47|      7.6|             Major|  116.0|  Intermediate| -6.2237|  146.471|\n",
      "|05-09-2022 04:52|      6.6|            Strong|   12.0|       Shallow| 29.7263|  102.279|\n",
      "|14-08-2022 13:44|      6.6|            Strong|   30.0|       Shallow|-32.6922| -178.959|\n",
      "|27-07-2022 00:43|      7.0|             Major| 33.729|       Shallow| 17.5978|  120.809|\n",
      "|08-06-2022 00:55|      6.5|            Strong| 622.73|          Deep| -9.0618| -71.1647|\n",
      "|26-05-2022 12:02|      7.2|             Major|  236.0|  Intermediate|-14.8628| -70.3081|\n",
      "|19-05-2022 10:13|      6.9|            Strong|   10.0|       Shallow|-54.1325|  159.027|\n",
      "+----------------+---------+------------------+-------+--------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize SparkSession (assuming it's already done)\n",
    "# spark = SparkSession.builder.appName('basics').getOrCreate()\n",
    "\n",
    "# Assuming 'selected_data' is already a PySpark DataFrame\n",
    "\n",
    "# Define the UDFs for Magnitude and Depth categories\n",
    "@udf(StringType())\n",
    "def magnitude_category(magnitude):\n",
    "    if magnitude < 5:\n",
    "        return \"Light\"\n",
    "    elif 5 <= magnitude < 6:\n",
    "        return \"Moderate\"\n",
    "    elif 6 <= magnitude < 7:\n",
    "        return \"Strong\"\n",
    "    elif 7 <= magnitude < 8:\n",
    "        return \"Major\"\n",
    "    else:\n",
    "        return \"Great\"\n",
    "\n",
    "@udf(StringType())\n",
    "def depth_category(depth):\n",
    "    if depth < 70:\n",
    "        return \"Shallow\"\n",
    "    elif 70 <= depth < 300:\n",
    "        return \"Intermediate\"\n",
    "    else:\n",
    "        return \"Deep\"\n",
    "\n",
    "# Apply the UDFs to create the new columns\n",
    "selected_data = selected_data.withColumn('Magnitude Category', magnitude_category(selected_data['magnitude']))\n",
    "selected_data = selected_data.withColumn('Depth Category', depth_category(selected_data['depth']))\n",
    "\n",
    "# Show the top rows with the new columns\n",
    "selected_data.select('date_time', 'magnitude', 'Magnitude Category', 'depth', 'Depth Category', 'latitude', 'longitude').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05d1d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-------+--------+---------+-------+----------------------------+-------------+--------------------+\n",
      "|       date_time|magnitude|  depth|latitude|longitude|tsunami|number_of_buildings_impacted|economic_loss|            location|\n",
      "+----------------+---------+-------+--------+---------+-------+----------------------------+-------------+--------------------+\n",
      "|22-11-2022 02:03|      7.0|   14.0| -9.7963|  159.596|      1|                        1502|     10790000|    Malango, Islands|\n",
      "|18-11-2022 13:37|      6.9|   25.0| -4.9559|  100.738|      0|                        2587|     28900000| Bengkulu, Indonesia|\n",
      "|12-11-2022 07:09|      7.0|  579.0|-20.0508| -178.346|      1|                        2654|     23040000|                null|\n",
      "|11-11-2022 10:48|      7.3|   37.0|-19.2918| -172.129|      1|                        1056|     23020000|       Neiafu, Tonga|\n",
      "|09-11-2022 10:14|      6.6|624.464|-25.5948|  178.278|      1|                         706|      1080000|                null|\n",
      "|09-11-2022 09:51|      7.0|  660.0|-26.0442|  178.381|      1|                         107|     40270000|   the Fiji, Islands|\n",
      "|09-11-2022 09:38|      6.8|630.379|-25.9678|  178.363|      1|                         590|     41340000|   the Fiji, Islands|\n",
      "|20-10-2022 11:57|      6.7|   20.0|  7.6712| -82.3396|      1|                        2469|     32070000|  Boca Chica, Panama|\n",
      "|22-09-2022 06:16|      6.8|   20.0|   18.33| -102.913|      1|                        2414|     11840000|   Aguililla, Mexico|\n",
      "|19-09-2022 18:05|      7.6| 26.943| 18.3667| -103.252|      1|                        1601|     38320000|   Aguililla, Mexico|\n",
      "|18-09-2022 06:44|      6.9|   10.0| 23.1444|  121.307|      1|                        2465|      3190000|      Yujing, Taiwan|\n",
      "|17-09-2022 13:41|      6.5|   10.0|  23.029|  121.348|      1|                         229|     34900000|        Lugu, Taiwan|\n",
      "|14-09-2022 11:04|      7.0|  137.0|-21.2077|  170.239|      1|                         916|     32800000|    Isangel, Vanuatu|\n",
      "|10-09-2022 23:47|      7.6|  116.0| -6.2237|  146.471|      1|                         795|      6070000|    Kainantu, Guinea|\n",
      "|05-09-2022 04:52|      6.6|   12.0| 29.7263|  102.279|      0|                        3022|     20940000|     Kangding, China|\n",
      "|14-08-2022 13:44|      6.6|   30.0|-32.6922| -178.959|      1|                        3544|     19750000|the Kermadec, Isl...|\n",
      "|27-07-2022 00:43|      7.0| 33.729| 17.5978|  120.809|      1|                        1074|     43770000| Bantay, Philippines|\n",
      "|08-06-2022 00:55|      6.5| 622.73| -9.0618| -71.1647|      0|                        3352|     23710000|    Tarauacá, Brazil|\n",
      "|26-05-2022 12:02|      7.2|  236.0|-14.8628| -70.3081|      1|                        1745|     13770000|      Azángaro, Peru|\n",
      "|19-05-2022 10:13|      6.9|   10.0|-54.1325|  159.027|      1|                        1085|     41250000|                null|\n",
      "+----------------+---------+-------+--------+---------+-------+----------------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the infrastructure_damage data from CSV\n",
    "infrastructure_damage = spark.read.csv('Dataset/infrastructure_damage.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Join the two dataframes on the 'date_time' column\n",
    "merged_df = selected_data.join(infrastructure_damage, on='date_time', how='inner')\n",
    "# Drop the unwanted columns\n",
    "merged_ds = merged_df.drop(\"Unnamed: 4\", \"Unnamed: 5\")\n",
    "\n",
    "# Display the merged data\n",
    "merged_ds.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dd1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+---------+----------------+----------------------------+-------------+-------+\n",
      "|magnitude|  depth|latitude|longitude|       date_time|number_of_buildings_impacted|economic_loss|tsunami|\n",
      "+---------+-------+--------+---------+----------------+----------------------------+-------------+-------+\n",
      "|      7.0|   14.0| -9.7963|  159.596|22-11-2022 02:03|                        1502|     10790000|      1|\n",
      "|      6.9|   25.0| -4.9559|  100.738|18-11-2022 13:37|                        2587|     28900000|      0|\n",
      "|      7.0|  579.0|-20.0508| -178.346|12-11-2022 07:09|                        2654|     23040000|      1|\n",
      "|      7.3|   37.0|-19.2918| -172.129|11-11-2022 10:48|                        1056|     23020000|      1|\n",
      "|      6.6|624.464|-25.5948|  178.278|09-11-2022 10:14|                         706|      1080000|      1|\n",
      "|      7.0|  660.0|-26.0442|  178.381|09-11-2022 09:51|                         107|     40270000|      1|\n",
      "|      6.8|630.379|-25.9678|  178.363|09-11-2022 09:38|                         590|     41340000|      1|\n",
      "|      6.7|   20.0|  7.6712| -82.3396|20-10-2022 11:57|                        2469|     32070000|      1|\n",
      "|      6.8|   20.0|   18.33| -102.913|22-09-2022 06:16|                        2414|     11840000|      1|\n",
      "|      7.6| 26.943| 18.3667| -103.252|19-09-2022 18:05|                        1601|     38320000|      1|\n",
      "|      6.9|   10.0| 23.1444|  121.307|18-09-2022 06:44|                        2465|      3190000|      1|\n",
      "|      6.5|   10.0|  23.029|  121.348|17-09-2022 13:41|                         229|     34900000|      1|\n",
      "|      7.0|  137.0|-21.2077|  170.239|14-09-2022 11:04|                         916|     32800000|      1|\n",
      "|      7.6|  116.0| -6.2237|  146.471|10-09-2022 23:47|                         795|      6070000|      1|\n",
      "|      6.6|   12.0| 29.7263|  102.279|05-09-2022 04:52|                        3022|     20940000|      0|\n",
      "|      6.6|   30.0|-32.6922| -178.959|14-08-2022 13:44|                        3544|     19750000|      1|\n",
      "|      7.0| 33.729| 17.5978|  120.809|27-07-2022 00:43|                        1074|     43770000|      1|\n",
      "|      6.5| 622.73| -9.0618| -71.1647|08-06-2022 00:55|                        3352|     23710000|      0|\n",
      "|      7.2|  236.0|-14.8628| -70.3081|26-05-2022 12:02|                        1745|     13770000|      1|\n",
      "|      6.9|   10.0|-54.1325|  159.027|19-05-2022 10:13|                        1085|     41250000|      1|\n",
      "+---------+-------+--------+---------+----------------+----------------------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the columns you want to select\n",
    "selected_columns = ['magnitude', 'depth', 'latitude', \n",
    "                    'longitude', 'date_time', \n",
    "                    'number_of_buildings_impacted', \n",
    "                    'economic_loss', 'tsunami']\n",
    "\n",
    "# Select the desired columns from the merged DataFrame\n",
    "reduced_data = merged_ds.select(*selected_columns)\n",
    "\n",
    "# Display the reduced data\n",
    "reduced_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553f8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the data\n",
    "output_path = \"BDAS/Dataset\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "reduced_data.write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aa083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
